{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/LunaHub/Friends_social_data_analysis_2019/master/data/All_Friends_data.csv\"\n",
    "df = pd.read_csv(url, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for clearing lines\n",
    "def cleanLines(line):\n",
    "    #Exclude links\n",
    "    line = re.sub(r'\\([^)]*\\)','',line)\n",
    "    #Remove everything besides letters\n",
    "    line = re.sub(\"([^a-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", line)\n",
    "    #Create a string for tweet\n",
    "    line = [''.join(s for s in line)]\n",
    "    return(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create list of real words\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add words to stopwords\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [\"like\",\"get\",\"yeah\",\"know\",\"gonna\",\"hey\",\"uh\",\"oh\",\"well\",\"okay\",\"im\",\"dont\",\"youre\",\"thats\",\"got\"]\n",
    "stopWords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find episodes  of each season\n",
    "s1 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(1[0-2][1-9])')].reset_index(drop=True)\n",
    "s2 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(2[0-2][1-9])')].reset_index(drop=True)\n",
    "s3 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(3[0-2][1-9])')].reset_index(drop=True)\n",
    "s4 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(4[0-2][1-9])')].reset_index(drop=True)\n",
    "s5 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(5[0-2][1-9])')].reset_index(drop=True)\n",
    "s6 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(6[0-2][1-9])')].reset_index(drop=True)\n",
    "s7 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(7[0-2][1-9])')].reset_index(drop=True)\n",
    "s8 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(8[0-2][1-9])')].reset_index(drop=True)\n",
    "s9 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(9[0-2][1-9])')].reset_index(drop=True)\n",
    "s10 = df[['Speaker','Text','Scene']][df['Episode'].str.match(pat = '(10[0-2][1-9])')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mainch = [\"joey\",\"rachel\",\"ross\",\"chandler\",\"monica\",\"phoebe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create list for what words the characters uses in the seasons\n",
    "allwords = []\n",
    "liness1 = []\n",
    "for character in mainch:\n",
    "    says = s1.Text[[character in i for i in s1.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness1.extend(words)\n",
    "allwords.extend(liness1)\n",
    "    \n",
    "liness2 = []\n",
    "for character in mainch:\n",
    "    says = s2.Text[[character in i for i in s2.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness2.extend(words)\n",
    "allwords.extend(liness2)\n",
    "        \n",
    "liness3 = []\n",
    "for character in mainch:\n",
    "    says = s3.Text[[character in i for i in s3.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness3.extend(words)\n",
    "allwords.extend(liness3)\n",
    "\n",
    "liness4 = []\n",
    "for character in mainch:\n",
    "    says = s4.Text[[character in i for i in s4.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness4.extend(words)\n",
    "allwords.extend(liness4)\n",
    "\n",
    "liness5 = []\n",
    "for character in mainch:\n",
    "    says = s5.Text[[character in i for i in s5.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness5.extend(words)\n",
    "allwords.extend(liness5)\n",
    "\n",
    "liness6 = []\n",
    "for character in mainch:\n",
    "    says = s6.Text[[character in i for i in s6.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness6.extend(words)\n",
    "allwords.extend(liness6)\n",
    "    \n",
    "liness7 = []\n",
    "for character in mainch:\n",
    "    says = s7.Text[[character in i for i in s7.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness7.extend(words)\n",
    "allwords.extend(liness7)\n",
    "\n",
    "liness8 = []\n",
    "for character in mainch:\n",
    "    says = s8.Text[[character in i for i in s8.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness8.extend(words)\n",
    "allwords.extend(liness8)\n",
    "    \n",
    "liness9 = []\n",
    "for character in mainch:\n",
    "    says = s9.Text[[character in i for i in s9.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness9.extend(words)\n",
    "allwords.extend(liness9)\n",
    "\n",
    "liness10 = []\n",
    "for character in mainch:\n",
    "    says = s10.Text[[character in i for i in s10.Speaker.str.split(\"\\b\")]].reset_index(drop=True)\n",
    "    for i in range(len(says)):\n",
    "        words = [w for w in says[i].split() if w in english_vocab and w not in stopWords]\n",
    "        liness10.extend(words)\n",
    "allwords.extend(liness10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/LunaHub/Friends_social_data_analysis_2019/master/data/Data_Set_S1.csv\"\n",
    "s1 = pd.read_csv(url, sep='\\t')\n",
    "# Sentiment dictonary:\n",
    "sentiments = dict(zip(s1.word,s1.happiness_average))\n",
    "# Function that calculates sentiment given a list of tokens\n",
    "def sentiment_func(token_list):\n",
    "    res = []\n",
    "    for word in token_list:\n",
    "        if word in s1.word.values:\n",
    "            res.append(sentiments[word])\n",
    "        \n",
    "    return sum(res)/len(res) if len(res) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentiment for season 1\n",
    "s1sent = []\n",
    "for line in liness1:\n",
    "    s1sent.append(sentiment_func(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clear out 0 instances\n",
    "s1Sent = [i for i in s1sent if i > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment for season 2\n",
    "s2sent = []\n",
    "for line in liness2:\n",
    "    s2sent.append(sentiment_func(line))\n",
    "\n",
    "#Sentiment for season 3\n",
    "s3sent = []\n",
    "for line in liness3:\n",
    "    s3sent.append(sentiment_func(line))\n",
    "    \n",
    "    \n",
    "#Sentiment for season 4\n",
    "s4sent = []\n",
    "for line in liness4:\n",
    "    s4sent.append(sentiment_func(line))\n",
    "\n",
    "#Sentiment for season 5\n",
    "s5sent = []\n",
    "for line in liness5:\n",
    "    s5sent.append(sentiment_func(line))\n",
    "\n",
    "#Sentiment for season 6\n",
    "s6sent = []\n",
    "for line in liness6:\n",
    "    s6sent.append(sentiment_func(line))\n",
    "    \n",
    "    \n",
    "#Sentiment for season 7\n",
    "s7sent = []\n",
    "for line in liness7:\n",
    "    s7sent.append(sentiment_func(line))\n",
    "\n",
    "#Sentiment for season 8\n",
    "s8sent = []\n",
    "for line in liness8:\n",
    "    s8sent.append(sentiment_func(line))\n",
    "\n",
    "#Sentiment for season 9\n",
    "s9sent = []\n",
    "for line in liness9:\n",
    "    s9sent.append(sentiment_func(line))\n",
    "    \n",
    "    \n",
    "#Sentiment for season 10\n",
    "s10sent = []\n",
    "for line in liness10:\n",
    "    s10sent.append(sentiment_func(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clear out 0 instances\n",
    "s2Sent = [i for i in s2sent if i > 0]\n",
    "s3Sent = [i for i in s3sent if i > 0]\n",
    "s4Sent = [i for i in s4sent if i > 0]\n",
    "s5Sent = [i for i in s5sent if i > 0]\n",
    "s6Sent = [i for i in s6sent if i > 0]\n",
    "s7Sent = [i for i in s7sent if i > 0]\n",
    "s8Sent = [i for i in s8sent if i > 0]\n",
    "s9Sent = [i for i in s9sent if i > 0]\n",
    "s10Sent = [i for i in s10sent if i > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot:\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.kdeplot(s1Sent, label=\"Season 1\")\n",
    "sns.kdeplot(s2Sent, label=\"Season 2\")\n",
    "sns.kdeplot(s3Sent, label=\"Season 3\")\n",
    "sns.kdeplot(s4Sent, label=\"Season 4\")\n",
    "sns.kdeplot(s5Sent, label=\"Season 5\")\n",
    "plt.legend();\n",
    "plt.xlabel('Sentiment');\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Distribution of sentiment');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.kdeplot(s6Sent, label=\"Season 6\")\n",
    "sns.kdeplot(s7Sent, label=\"Season 7\")\n",
    "sns.kdeplot(s8Sent, label=\"Season 8\")\n",
    "sns.kdeplot(s9Sent, label=\"Season 9\")\n",
    "sns.kdeplot(s10Sent, label=\"Season 10\")\n",
    "plt.legend();\n",
    "plt.xlabel('Sentiment');\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Distribution of sentiment');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
